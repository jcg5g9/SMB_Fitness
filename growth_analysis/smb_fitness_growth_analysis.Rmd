---
title: "Analysis 4: von Bertalanffy Individual Growth Analysis"
author: "Joe Gunn"
date: "2023-04-28"
output: html_document
---

# Project: Effects of admixture on fitness in Neosho Bass populations 
<font size="+1">We assessed the effect of admixture on fitness in two stream populations within the native range of the Neosho Bass (<i>M. velox</i>) which are known to have extensively hybridized with Smallmouth Bass (<i>Micropterus dolomieu</i>). Specifically, we used 14 microsatellite loci in a Bayesian analysis of population structure to estimate proportions of interspecific ancestry in individuals collected from Big Sugar Creek and the Elk River in southwestern Missouri (Central Interior Highlands ecoregion (CIH), North America). We used ancestry inference to identify fish as "Pure Neosho Bass", "Pure Smallmouth Bass", or "Admixed". For each group, we measured age and total length and projected individual growth using the standard paramaterization of the von Berlanffy growth model, comparing average theoretical maximum length among groups. Finally, we used body condition as a proxy of fitness and generated heterozygosity-fitness correlations of body condition across the global dataset, within stream populations, and within ancestry groups. We ultimately sought to understand the short-term genetic consequences of admixture for Neosho Bass populations in order to better inform management and long-term viability of distinct, economically and ecologically important sportfish species in the CIH.</font>

## Specific Aim: von Bertalanffy individual growth analysis
For this aim, paramaterized the von Bertlanffy individual growth model for Neosho Bass in Big Sugar Creek and Elk River based on total length (tl_alive) and consensus age data to assess growth rates in inferred groups of interest. We started by using the Dahl-Lea linear back-calculation model We started by quantifying overall growth rate and maximum theoretical total length for all fish collected across both streams. Next, we quantified potential differences in growth by (1) individual sex (male or female) and (2) by stream (Big Sugar Creek or Elk River) to account for variation which may bias results in other groups. Finally, we assessed differences in growth based on ancestry group membership as inferred by STRUCTURE analysis in Analysis 3 (Neosho Bass, Smallmouth Bass, Admixed). For all growth assessments, we included linear back-calculated length-at-age estimates for large, older fish to increase sample size.

## Phases of Analysis
### Phase 1: Intra- and inter-observer error analysis for otolith annuli estimates
### Phase 1: Linear back-calculation
### Phase 2: von Bertlanffy growth analysis

### Libraries needed for analysis
```{r}
library(tidyverse)
library(cowplot)
library(readxl)
library(RFishBC)
library(rlist)
library(rstan)
library(bayesplot)
library(GGally)
```

## PHASE 1: INTRA- AND INTER-OBSERVER ERROR ANALYSIS FOR OTOLITH ANNULI ESTIMATES
In this phase of analysis, we conduct an analysis of intra- and inter-observer bias in estimating the number of otolith annuli on raw otolith microscope images. Three independent observers estimated the number annuli on each otolith image, and counts were compared among observers. If all observers agreed on annuli count for a given image, that count was used as the "consensus age". If there were discrepancies among two or three observers, a consensus age was determined by discussion.

### STEP 1: Select sets of otolith images consisting of a random 10% of the full dataset, to be re-counted by each observer (3 total observers); run the Rmd chunk below.
In this step, we selected sets of images at random, each including 10% of the original dataset (10% of 116 ~ 12 images per observer).

##### Select random sets of images:
```{r}
# Get vector of image names
otos <- c("FBS01.PNG","FBS02.PNG","FBS03.PNG","FBS04.PNG","FBS05.PNG","FBS06.PNG","FBS07.PNG","FBS08.PNG","FBS09.PNG","FBS10.PNG","FBS11.PNG","FBS12.PNG","FBS13.PNG","FBS14.PNG","FBS15.PNG","FBS16.PNG","FBS17.PNG","FBS18.PNG","FBS19.PNG","FBS20.PNG","FBS21.PNG","FBS22.PNG","FBS23.PNG","FBS24.PNG","FBS25.PNG","FBS26.PNG","FBS27.PNG","FBS28.PNG","FBS29.PNG","FBS30.PNG","FBS31.PNG","FBS32.PNG","FBS33.PNG","FBS34.PNG","FBS35.PNG","FBS36.PNG","FBS37.PNG","FBS38.PNG","FBS39.PNG","FBS40.PNG","FBS41.PNG","FBS42.PNG","FBS43.PNG","FBS44.PNG","FBS45.PNG","FBS46.PNG","FER01.PNG","FER02.PNG","FER03.PNG","FER04.PNG","FER05.PNG","FER06.PNG","FER07.PNG","FER08.PNG","FER09.PNG","FER10.PNG","FER11.PNG","FER12.PNG","FER13.PNG","FER14.PNG","FER15.PNG","FER16.PNG","FER17.PNG","FER18.PNG","FER19.PNG","FER20.PNG","FER21.PNG","FER22.PNG","FER23.PNG","FER24.PNG","FER25.PNG","FER26.PNG","FER27.PNG","FER28.PNG","FER29.PNG","FER30.PNG","FER31.PNG","FER32.PNG","FER33.PNG","FER34.PNG","FER35.PNG","FER36.PNG","FER37.PNG","FER38.PNG","FER39.PNG","FER40.PNG","FER41.PNG","FER42.PNG","FER43.PNG","FER44.PNG","FER45.PNG","FER46.PNG","FER48.PNG","FER49.PNG","FER50.PNG","FER51.PNG","FER52.PNG","FER53.PNG","FER54.PNG","FER55.PNG","FER56.PNG","FER57.PNG","FER58.PNG","FER59.PNG","FER60.PNG","FER61.PNG","FER62.PNG","FER63.PNG","FER64.PNG","FER65.PNG","FER66.PNG","FER68.PNG","FER69.PNG","FER70.PNG","FER71.PNG")

# Select 12 random images for recount by each observer
sample(otos, size = 12, replace = FALSE)
```

<b>Summary of sample sets for observers</b>:

<b>Joe</b>: FBS07.PNG, FER03.PNG, FBS40.PNG, FER39.PNG, FBS15.PNG, FER31.PNG, FBS17.PNG, FBS44.PNG, FER41.PNG, FBS37.PNG, FER65.PNG, FER11.PNG

<b>Eddie</b>: FBS07.PNG, FER06.PNG, FER19.PNG, FER07.PNG, FER20.PNG, FER35.PNG, FER43.PNG, FBS38.PNG, FER37.PNG, FBS06.PNG, FER55.PNG, FER16.PNG

<b>Michael</b>: FER54.PNG, FER48.PNG, FER33.PNG, FER34.PNG, FBS09.PNG, FBS03.PNG, FER62.PNG, FER60.PNG, FBS25.PNG, FBS24.PNG, FER70.PNG, FBS15.PNG

### STEP 2: Calculate intra-observer error; run the Rmd chunk below.
We calculated intra-observer error for each observer individually. We opted to calculate error as the number of discrepancies (number of individual samples for which there was a difference between the original annulus count on the sagittal otolith and the recount (which was conducted after 2 years)) divided by the total number of comparisons. 

Each observer recounted 12 randomly assigned samples (see STEP 1 above), so error was calculated as: error = discrepancies/12

We then calculated the mean of intra-observer error values across observers and reported this error in the final manuscript.

##### Calculate intra-observer error:
```{r}
# Load in full ancestry data curated in Analysis 3 
load("../ancestry_analysis/data/processed_ancestry_data/full_ancestry_data.Rda")

# Read in recount data for each observer
eddie_recount <- read_excel("data/otolith_recounts/eddie_recount.xlsx")
joe_recount <- read_excel("data/otolith_recounts/joe_recount.xlsx")
michael_recount <- read_excel("data/otolith_recounts/michael_recount.xlsx")

# format recount data
eddie_recount <- eddie_recount %>%
  mutate(sample_id = factor(sample_id))

joe_recount <- joe_recount %>%
  mutate(sample_id = factor(sample_id))

michael_recount <- michael_recount %>%
  mutate(sample_id = factor(sample_id))

# Get original count data from each observer
eddie_original <- full_ancestry_data %>%
  select(sample_id, eddie_age)

joe_original <- full_ancestry_data %>%
  select(sample_id, joe_age)

michael_original <- full_ancestry_data %>%
  select(sample_id, michael_age)

# merge original and recount data
eddie <- merge(eddie_original, 
               eddie_recount, 
               by = "sample_id")

# rename second column
colnames(eddie)[2] <- c("age")

joe <- merge(joe_original, 
               joe_recount, 
               by = "sample_id")

# rename second column
colnames(joe)[2] <- c("age")

michael <- merge(michael_original, 
               michael_recount, 
               by = "sample_id")

# rename second column
colnames(michael)[2] <- c("age")


# Create column in recount data to represent difference between original age and recount estimates (0 if there are no differences, and 1 if there is a difference)
michael$diff <- as.numeric(with(michael, ifelse(age==recount, "0", "1")))
joe$diff <- as.numeric(with(joe, ifelse(age==recount, "0", "1")))
eddie$diff <- as.numeric(with(eddie, ifelse(age==recount, "0", "1")))


# Calcualte intra-observer error by dividing the number of count discrepancies divided by the total number of observations
michael_error <- sum(michael$diff)/(nrow(michael))
joe_error <- sum(joe$diff)/(nrow(joe))
eddie_error <- sum(eddie$diff)/(nrow(eddie))

# Calculate average error
average_error <- (michael_error + joe_error + eddie_error)/3
```

We calculated an average intra-observer error of 0.05555556.

### STEP 2: Calculate inter-observer error; run the Rmd chunk below.
We calculated inter-observer error between each pair of observers (eddie and joe, eddie and michael, and michael and joe. We opted to calculate error as the number of discrepancies (number of individual samples for which there was a difference between the original observer annulus estimates) divided by the total number of comparisons. 

Each observer originally counted 116 samples (the full sample set), so error was calculated as: error = discrepancies/116

We then calculated the mean of inter-observer error values across inter-observer comparisons and reported this average error in the final manuscript.

##### Calculate intra-observer error:
```{r}
# Load in full ancestry data curated in Analysis 3 
load("../ancestry_analysis/data/processed_ancestry_data/full_ancestry_data.Rda")

# Get original count data for each observer
eddie_original <- full_ancestry_data %>%
  select(sample_id, eddie_age)

joe_original <- full_ancestry_data %>%
  select(sample_id, joe_age)

michael_original <- full_ancestry_data %>%
  select(sample_id, michael_age)

# bind columns of counts from each observer
originals <- data.frame(cbind("eddie" = eddie_original[,-1],
                   "joe" = joe_original[,-1], 
                   "michael" = michael_original[,-1]))

# Drop NAs from data (SMB samples without age estimates)
originals <- originals %>%
  drop_na()

# Create column in recount data to represent difference between original age and recount estimates (0 if there are no differences, and 1 if there is a difference)
originals$eddie_joe_diff <- as.numeric(with(originals, ifelse(eddie==joe, "0", "1")))
originals$eddie_michael_diff <- as.numeric(with(originals, ifelse(eddie==michael, "0", "1")))
originals$joe_michael_diff <- as.numeric(with(originals, ifelse(joe==michael, "0", "1")))

# Calcualte intra-observer error by dividing the number of count discrepancies divided by the total number of observations
eddie_joe_error <- sum(originals$eddie_joe_diff)/(nrow(originals))
eddie_michael_error <- sum(originals$eddie_michael_diff)/(nrow(originals))
joe_michael_error <- sum(originals$joe_michael_diff)/(nrow(originals))

# Calculate average error
average_error <- (eddie_joe_error + eddie_michael_error + joe_michael_error)/3
```

We calculated an average inter-observer error of 0.1494253

## PHASE 2: LINEAR BACK CALCULATION
In this phase of analysis, we conduct linear back-calculation on a subset of relatively high-age fish in our dataset to generate estimates of total length for fish at earlier ages. These estimates are not to be considered independent measurements of age and length, because they are calculated based on the assumption of a direct, one-to-one proportional relationship between an individual's total length and the radii of each annulus in the individual's otolith. Each back-calculated length is therefore an estimate of that individual's length at an earlier age. However, given a large sample size, these estimates can be used to approximate or represent lower age estimates within a population.

### STEP 1: Set "device type" in the package RFishBC (Ogle, 2022) so that the GUI will be readible by MAC OS; run the Rmd chunk below.
In this step, I needed to run a line of code built into the RFishBC package to set the computing device type to "X11", which allows the package's GUI to run on a MAC OS. 

##### Set device type to MAC (X11):
```{r}
RFBCoptions(deviceType = "X11")
```

### STEP 2: Copy raw otolith images into the working directory (`growth_analysis/`).
The RFishBC package requires that raw otolith images being read for back-calculation are placed in the working directory for the Rmd file, which, in this case, is the growth_analysis folder. We therefore *temporarily* copied all raw otolith camera images (`../raw_data/otolith_images/raw/`) into the current working directory.

### STEP 3: Estimate radial measurements of otolith annuli.
In this step, we run the function "digitizeRadii()" from the package RFishBC on each otolith image independently (we list the function for all unique image IDs), which allows interactive radial measurements by drawing radius transects directly on the image. The user is asked to (1) input a unique image ID, (2) draw a linear transect from the nucleus (centroid) to the outer edge of the otolith, and (3) identify each annulus along the transect to match the inferred age of the fish. One output of the function is a data table ("radii", an element of the .rds object) giving the total radius of the otolith and the radii for each individual annulus.

We input the ID given for each otolith (e.g., FBS01, FBS02, etc.). We drew transects from the centroid to the outer edge of the otolith on the same side and just above the sulcus (see Figure S2 in the final manuscript) to standardize radii for each otolith. We identified annuli by placing a reference point on the outermost edge of each annulus. For any otoliths that were cracked (and we could not reliably determine the radius from the nucleus to an otolith edge), cloudy or obscured, or without clearly defined edges, we did not include those samples, and they are indicated within the code chunk.

#### 3a: Estimate radial measurements for Big Sugar Creek samples; run the Rmd chunk below:
In this step, we estimate radial measurements for the Big Sugar Creek samples only.

##### Run linear-back calculation on each image and generate output radius data:
```{r}
# Run linear back-calculation for Big Sugar Creek samples ("FBS")
digitizeRadii("FBS01.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS02.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS03.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS04.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS05.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS06.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS07.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS08.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS09.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS10.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS11.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS12.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS13.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS14.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS15.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS16.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS17.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS18.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS19.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS20.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS21.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS22.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS23.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS24.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS25.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS26.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS27.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS28.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS29.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS30.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS31.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS32.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS33.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS34.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS35.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS36.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS37.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS38.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS39.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS40.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS41.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS42.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS43.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS44.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS45.PNG", edgeIsAnnulus = F)
digitizeRadii("FBS46.PNG", edgeIsAnnulus = F)
```

<b>Big Sugar Creek samples used for back-calculation</b>: <i>N</i> = 46

#### 3b: Estimate radial measurements for Elk River samples; run the Rmd chunk below:
In this step, we estimate radial measurements for the Elk River samples only.

```{r}
# Run linear back-calculation for Elk River samples ("FER")
digitizeRadii("FER01.PNG", edgeIsAnnulus = F)
digitizeRadii("FER02.PNG", edgeIsAnnulus = F)
digitizeRadii("FER03.PNG", edgeIsAnnulus = F)
digitizeRadii("FER04.PNG", edgeIsAnnulus = F)
digitizeRadii("FER05.PNG", edgeIsAnnulus = F)
digitizeRadii("FER06.PNG", edgeIsAnnulus = F)
digitizeRadii("FER07.PNG", edgeIsAnnulus = F)
#digitizeRadii("FER08.PNG", edgeIsAnnulus = F) ## Cracked otolith, unable to reliably determine radius from nucleus to edge
digitizeRadii("FER09.PNG", edgeIsAnnulus = F)
digitizeRadii("FER10.PNG", edgeIsAnnulus = F)
digitizeRadii("FER11.PNG", edgeIsAnnulus = F)
digitizeRadii("FER12.PNG", edgeIsAnnulus = F)
digitizeRadii("FER13.PNG", edgeIsAnnulus = F)
digitizeRadii("FER14.PNG", edgeIsAnnulus = F)
digitizeRadii("FER15.PNG", edgeIsAnnulus = F)
digitizeRadii("FER16.PNG", edgeIsAnnulus = F)
digitizeRadii("FER17.PNG", edgeIsAnnulus = F)
digitizeRadii("FER18.PNG", edgeIsAnnulus = F)
digitizeRadii("FER19.PNG", edgeIsAnnulus = F)
digitizeRadii("FER20.PNG", edgeIsAnnulus = F)
digitizeRadii("FER21.PNG", edgeIsAnnulus = F)
digitizeRadii("FER22.PNG", edgeIsAnnulus = F)
digitizeRadii("FER23.PNG", edgeIsAnnulus = F)
digitizeRadii("FER24.PNG", edgeIsAnnulus = F)
digitizeRadii("FER25.PNG", edgeIsAnnulus = F) 
digitizeRadii("FER26.PNG", edgeIsAnnulus = F)
digitizeRadii("FER27.PNG", edgeIsAnnulus = F)
digitizeRadii("FER28.PNG", edgeIsAnnulus = F) 
digitizeRadii("FER29.PNG", edgeIsAnnulus = F)
digitizeRadii("FER30.PNG", edgeIsAnnulus = F) 
digitizeRadii("FER31.PNG", edgeIsAnnulus = F) 
digitizeRadii("FER32.PNG", edgeIsAnnulus = F)
digitizeRadii("FER33.PNG", edgeIsAnnulus = F)
digitizeRadii("FER34.PNG", edgeIsAnnulus = F)
digitizeRadii("FER35.PNG", edgeIsAnnulus = F)
digitizeRadii("FER36.PNG", edgeIsAnnulus = F)
digitizeRadii("FER37.PNG", edgeIsAnnulus = F)
digitizeRadii("FER38.PNG", edgeIsAnnulus = F)
digitizeRadii("FER39.PNG", edgeIsAnnulus = F)
digitizeRadii("FER40.PNG", edgeIsAnnulus = F)
digitizeRadii("FER41.PNG", edgeIsAnnulus = F)
digitizeRadii("FER42.PNG", edgeIsAnnulus = F)
digitizeRadii("FER43.PNG", edgeIsAnnulus = F)
digitizeRadii("FER44.PNG", edgeIsAnnulus = F)
digitizeRadii("FER45.PNG", edgeIsAnnulus = F)
digitizeRadii("FER46.PNG", edgeIsAnnulus = F)
#digitizeRadii("FER47.PNG", edgeIsAnnulus = F) # Cracked otolith, missing sulcus, unable to reliably determine radius from nucleus to edge
digitizeRadii("FER48.PNG", edgeIsAnnulus = F)
digitizeRadii("FER49.PNG", edgeIsAnnulus = F)
digitizeRadii("FER50.PNG", edgeIsAnnulus = F)
digitizeRadii("FER51.PNG", edgeIsAnnulus = F)
digitizeRadii("FER52.PNG", edgeIsAnnulus = F)
digitizeRadii("FER53.PNG", edgeIsAnnulus = F)
digitizeRadii("FER54.PNG", edgeIsAnnulus = F) 
digitizeRadii("FER55.PNG", edgeIsAnnulus = F)
digitizeRadii("FER56.PNG", edgeIsAnnulus = F)
digitizeRadii("FER57.PNG", edgeIsAnnulus = F)
digitizeRadii("FER58.PNG", edgeIsAnnulus = F)
digitizeRadii("FER59.PNG", edgeIsAnnulus = F)
digitizeRadii("FER60.PNG", edgeIsAnnulus = F)
digitizeRadii("FER61.PNG", edgeIsAnnulus = F)
digitizeRadii("FER62.PNG", edgeIsAnnulus = F)
digitizeRadii("FER63.PNG", edgeIsAnnulus = F)
digitizeRadii("FER64.PNG", edgeIsAnnulus = F)
digitizeRadii("FER65.PNG", edgeIsAnnulus = F)
digitizeRadii("FER66.PNG", edgeIsAnnulus = F)
digitizeRadii("FER68.PNG", edgeIsAnnulus = F)
digitizeRadii("FER69.PNG", edgeIsAnnulus = F)
#digitizeRadii("FER70.PNG", edgeIsAnnulus = F) # Poor image quality
digitizeRadii("FER71.PNG", edgeIsAnnulus = F)
```

<b>Elk River samples used for back-calculation</b>: <i>N</i> = 67
<b>Total samples used for back-calculation</b>: <i>N</i> = 113

#### 3c: Remove all copied raw otolith images from the working directory.

#### 3d: Move all .rds files generated from the digitizeRadii() function into a new folder: `data/bc_rds/`

### STEP 4: Compile and clean all radius measurements stored in .rds files
In this step, we wrote a for-loop to extract the "radii" elements from each of the .rds files generated by the digitizeRadii() function used in Step 3 above. All radii data were for each sample were then compiled into a single dataframe ("bc"). We then cleaned, filtered, and merged the bc data with total length data to perform linear back-calculation of length-at-age.

#### 4a: Extract radial measurements for each sample from .rds files and compile into a single data frame for back-calculation

##### Extract radial measurements and compile into single dataframe:
```{r}
# List rds filers
rds <- list.files(paste("data/bc_rds/"),
                  pattern=".rds$")

# Generate data frame of rds file names
rds_files <- data_frame(rds)

# Generate an empty list to store "radii" data frame elements from .rds files
rds_list <- list()

# Run for loop on all .rds files to extract radii data frame and add to the rds_list (defined above)
for(ii in 1:nrow(rds_files)) {
  
  ## paste the readRDS() function so that it calls each .rds file in the rds files data frame
  read_rds <- readRDS(paste("data/bc_rds/", rds_files[ii,"rds"], sep = ""))$radii
  
  ## Fill the empty list with the iterative output of each read_rds object (defined above)
  rds_list[[paste0("element",ii)]] <- read_rds
  
}

# Concatenate all data frame elements in the list using the list.rbind() function from the package "rlist"
bc <- data.frame(list.rbind(rds_list))
```

#### 4b: Clean and filter radial measurement data for downstream back-calculation; run the Rmd chunk below.
In this step, we clean and filter the radial measurement data so that it can be seamlessly merged with the full ancestry dataset (See Step 5).

Data from the compiled rds files include: 

   1. "sample_id": same as in metadata and genotype data (followed by "structure_number"; see Analysis 3, Phase 1, step 1e)
   2. "consensus age": ultimate age estimate based on consensus of three agers, Eddie, Joe, and Michael 
   3. "annulus": each annulus for each otolith
   4. "annulus_radius": radius of each annulus 
   5. "otolith_radius": total radius of the whole otolith

##### Clean and filter, and merge radial measurement data:
```{r}
# Omit rownames
rownames(bc) <- NULL

# Omit empty "reading" column
bc <- bc %>%
  select(-c(reading))

# Modify column names to be more sensible
colnames(bc) <- c("sample_id", "consensus_age", "annulus", "annulus_radius", "otolith_radius")

# Here, I added a row for each of the samples we did not do radius measurements for. I included these as individual rows, because without them, the merging procedure in step 5 results in "NA" values for these individuals since they do not have radial measurements and cannot be used for back-calculation.
bc[451,] <- c("FER08", "2", "2", "1.0000000", "1.0000000")
bc[452,] <- c("FER47", "2", "2", "1.0000000", "1.0000000")
bc[453,] <- c("FER70", "2", "2", "1.0000000", "1.0000000")

# Convert characters to factors
bc <- bc %>%
  mutate(sample_id = factor(sample_id)) %>%
  mutate(consensus_age = as.numeric(consensus_age)) %>%
  mutate(annulus = as.numeric(annulus)) %>%
  mutate(annulus_radius = as.numeric(annulus_radius)) %>%
  mutate(otolith_radius = as.numeric(otolith_radius))

# Save radius data for donwtream analyses
save(bc, file = "data/bc_data/bc.Rda")
```

### STEP 5: Merge radius data with full ancestry data (Analysis 3) and perform linear back-calculation 
In this step, we merge the annuli radii data generated above with the full ancestry data generated in Analysis 3, and we perform linear back-calculation using the Dahl-Lea direct proportion model with total length. We estimate length-at-age for each individual fish (age of the fish at all earlier ages) based on the assumed proportional relationship between total length of the fish at a given age and the radius of the fish at the corresponding annulus deposition.

#### 5a: Merge radius data with full ancestry data; run the Rmd chunk below.

##### Merge radius data with full ancestry data:
```{r}
# Load in full ancestry data curated in Analysis 3 
load("../ancestry_analysis/data/processed_ancestry_data/full_ancestry_data.Rda")

# Filter dataset to keep only fish with total length and consensus age data ("sample"; NOT "reference")
full_ancestry_data <- full_ancestry_data %>%
  filter(population == "sample")

# Load in bc data calculated in Steps 3-4 above
load("data/bc_data/bc.Rda")

# Merge full ancestry data with bc data 
bc_data <- full_join(full_ancestry_data, 
                     bc, 
                     by = c("sample_id", "consensus_age"))

# Perform back-calculation using the Dahl-Lee model
bc_data <- bc_data %>%
  mutate(bc_ratio = annulus_radius/otolith_radius) %>%
  mutate(bc_tl = tl_alive*(annulus_radius/otolith_radius))
```

#### 5b: Remove all length data corresponding to an annulus-to-otolith radial ratio of 1.00; run the Rmd chunk below.
In this step, we standardize all total length data such that the consensus age of the fish (determined by estimating the number of annuli deposited on each otolith) corresponds to the radial measurement of the last deposited otolith, rather than the edge of the otolith. For each fish, the margin of the otolith beyond the final annulus represents additional deposition of calcium carbonate over the course of the year; the exact timing of yearly deposition is dependent on the birth date (exact day) of the fish, which may be variable among fish in our sample. Thus, to standardize length-at-age, we only include back-calculated age data corresponding to each annulus, and not to the edge of the otolith. We therefore do not use the observed lengths of fish collected in the field.

From this point further, the column for "consensus_age", which was the age agreed upon for each fish based on the number of annuli present on the otolith, was considered synonymous with the column "annulus", with the highest annulus number for each sample being equivalent to the consensus age. Each lesser annulus number represents earlier ages for each fish, for which total length is back-calculated.

##### Remove fish with annulus-to-otolith radial ratio of 1.00:
```{r}
# Remove fish with annulus-to-otolith radial ratio of 1.00
full_bc_data <- bc_data %>%
  filter(bc_ratio != 1.0000000)

## Verify that the three samples not back-calculated ((FER08, FER47, FER70) were removed from the dataset

# Samples in full_bc_data but not in bc_data
full_bc_data %>% 
  filter(!full_bc_data$sample_id %in% bc_data$sample_id)

# Samples in bc_data but not in full_bc_data
bc_data %>% 
  filter(!bc_data$sample_id %in% full_bc_data$sample_id)

# Save full bc data for downstream growth analysis
save(full_bc_data, file = "data/bc_data/full_bc_data.Rda")
```

After this step, three samples (FER08, FER47, FER70) were omitted from analyses, because we were unable to measure radii for back-calculation (see Steps 3-4 above). These samples were not included in any growth models.

<b>Final back-calculation data summary</b>:
<i>N</i><sub>samples</sub> = 337

### STEP 6: Summarize full back-calculation data for the final manuscript; run the Rmd chunk below.
In this step, we summarize the full back-calculated dataset, which includes back-calculated length-at-annulus data points for all annuli for each fish in the dataset. 

##### Summarize full back-calculated dataset.
```{r}
# Load full bc data
load("data/bc_data/full_bc_data.Rda")

# Get total sample size of bc data
full_bc_data %>%
  count()

# Get sample size by river
full_bc_data %>%
  group_by(river) %>%
  count()

# Get sample size by sex
full_bc_data %>%
  group_by(sex) %>%
  count()

# Get sample size by river
full_bc_data %>%
  group_by(ancestry_group) %>%
  count()

# Get sample size for each annulus for the global dataset (number of fish with a length measurement at each annulus)
full_bc_data %>%
  group_by(annulus) %>%
  count()

# Get mean and standard deviation of back-calculated total length for the global dataset
full_bc_data %>%
  group_by(annulus) %>%
  summarize(mean_bctl = mean(bc_tl),
            sd_bctl = sd(bc_tl))

# Get sample size for each annulus for the stream datasets (number of fish with a length measurement at each annulus within each stream)
full_bc_data %>%
  group_by(annulus, river) %>%
  count()

# Get mean and standard deviation of back-calculated total length for each stream dataset
full_bc_data %>%
  group_by(annulus, river) %>%
  summarize(mean_bctl = mean(bc_tl),
            sd_bctl = sd(bc_tl))

# Get sample size for each annulus for the sex datasets (number of fish with a length measurement at each annulus within each sex)
full_bc_data %>%
  group_by(annulus, sex) %>%
  count()

# Get mean and standard deviation of back-calculated total length for each sex dataset
full_bc_data %>%
  group_by(annulus, sex) %>%
  summarize(mean_bctl = mean(bc_tl),
            sd_bctl = sd(bc_tl))

# Get sample size for each annulus for the ancestry group datasets (number of fish with a length measurement at each annulus within each ancestry group)
full_bc_data %>%
  group_by(annulus, ancestry_group) %>%
  count()

# Get mean and standard deviation of back-calculated total length for each sex dataset
full_bc_data %>%
  group_by(annulus, ancestry_group) %>%
  summarize(mean_bctl = mean(bc_tl),
            sd_bctl = sd(bc_tl))

```

<b>Summary of back-calculated length-at-age sample sizes</b>:

<b>Total sample size</b>: 337

<b><i>By river</i></b>:
<b>Big Sugar Creek</b>: 149
<b>Elk River</b>: 188

<b><i>By sex</i></b>:
<b>Male</b>: 183
<b>Female</b>: 154

<b><i>By ancestry group</i></b>:
<b>NB</b>: 134
<b>SMB</b>: 44
<b>ADM</b>: 159

### ----------------------- END OF PHASE 1: LINEAR BACK-CALCULATION  ----------------------- ###

## PHASE 2: VON BERTALANFFY GROWTH ANALYSIS
In this phase of the analysis, we use a Bayesian hierarchical framework to paramaterize the von Bertalanffy growth model using back-calculated total length and consensus age of samples in the Elk River and Big Sugar Creek to assess the contribution of non-native SMB ancestry to growth. We estimate the linear relationship between population-level growth parameters of the von Bertalanffy model (maximum theoretical total length-at-age, the Brody growth coefficient, and theoretical age at length-0) and ancestry proportion by quantifying average deviations from the global parameters due to SMB vs. NB ancestry, and we account for any potential differences in growth due to sex (male or female) or stream of origin (Big Sugar Creek or Elk River). For all growth models, we include individual ID as a random effect to account for individual variation in linear back-calculation estimates.

### STEP 1: Prepare data for Bayesian hierarhical analysis; run the Rmd code below.
In this step, we gather format all back-calculated length-at-age data generated in the back-calculation steps in Phase 1 (see above), including factors for stream population (Big Sugar Creek or Elk River) and sex (male or female). These data are then used as input for Bayesian hierarchical analysis. 

##### Load and prepare data for growth analysis:
```{r}
# Load full back-calculated data
load('data/bc_data/full_bc_data.rda')

# Set rstan parameters
rstan_options(threads_per_chain = 1)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()-1)

# Generate variables and match names to model 
full_bc_data <- full_bc_data %>%
  mutate(river_code = factor(river_code),
         river_code = ifelse(river_code == 3, 2, river_code),
         sex = as.numeric(factor(sex)))

age <- full_bc_data$annulus
length <- full_bc_data$bc_tl
group <- as.numeric(as.factor(full_bc_data$ancestry_group)) 
sample.id <- as.numeric(as.factor(as.character(full_bc_data$sample_id)))
Nind <- length(unique(sample.id))

# Set up model matrix (sex/river) where nrow = number of ind
model_matrix <- full_bc_data %>%
  group_by(sample_id) %>%
  slice(n()) %>%
  select(sample_id, smb, river_code, sex) %>%
  mutate(river_code = factor(river_code),
         sex = factor(sex)) %>%
  arrange(sample_id)

# Define data for model
bc_model_data = list(Nobs = length(length),
                     Nages = 15,
                     length = length,
                     age = age,
                     Zero = rep(0, 3),
                     
                     #Priors
                     eta_scale_prior = 0.5, 
                     cholesky_prior = 3,
                     beta_scale = 0.5,
                     
                     Nind = Nind,
                     Ncoef = 2,
                     X = model.matrix(~ sex + river_code, model_matrix)[,2:3],
                     Xhat = matrix(c(0,0,1,0,0,1,1,1), nrow = 4, ncol = 2, byrow = TRUE),
                     q = model_matrix$smb,
                     id = sample.id)

# Save data for back-calculated growth model
save(bc_model_data, file = "data/growth_model_data/bc_model_data.Rda")
```

### STEP 2: Run von Bertalanffy model for back-calculated length at age; run the Rmd chunk below.
In this step, we use a Bayesian hierarchical framework to fit a von Bertalanffy curve to back-calculated lenght-at-age data and account for SMB ancestry, sex, and stream of origin to assess the effect of SMB ancestry on growth parameters. 

##### Fit von Bertalanffy model:
```{r}
# Load data for back-calculated growth model
load("data/growth_model_data/bc_model_data.Rda")

# Set MCMC specifications
control = list(adapt_delta = 0.999, 
               stepsize = 0.001, 
               max_treedepth = 18)
warmup = 6000       
thin = 2
iter = 6000    

# Run back-calculated growth model
bc_model_fit <- stan(file = "code/bc_model.stan",  
  data = bc_model_data,    
  chains = 4,            
  warmup = warmup,      
  thin = thin,
  iter = iter * thin + warmup,     
  cores = 4,            
  control = control)

# Save back-calculation growth model fit output for downstream analyses
save(bc_model_fit, file = "data/growth_model_data/bc_model_fit.Rda")
```

### STEP 3: Summarize and plot model output from von Bertalanffy growth analysis with back-calculated data.
In this step, we analyze and plot output results from the generalized linear model of the condition and ancestry proportion.

#### 3a: Run diagnostics on model fit; run the Rmd chunk below:

##### Model diagnostics:
```{r}
# Load back-calculated model fit output
load("data/growth_model_data/bc_model_fit.Rda")

# Visually inspect chains
traceplot(bc_model_fit)

# Extract summary data for model fit  
sampler_params <- get_sampler_params(bc_model_fit, inc_warmup = TRUE)

# Summarize model fit
summary(do.call(rbind, sampler_params), digits = 2)


# Convergence diagnostics ----
#https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html

#Posterior ll
lp_cp <- log_posterior(bc_model_fit)
head(lp_cp)

#Accepted prob
np_cp <- nuts_params(bc_model_fit)
head(np_cp)

# Look at par vs divergence
color_scheme_set("darkgray")
posterior_cp <- as.array(bc_model_fit)
mcmc_parcoord(posterior_cp, np = np_cp)

#Look at ll vs acceptance
color_scheme_set("red")
mcmc_nuts_divergence(np_cp, lp_cp)

# Print and plot MCMC ----
print(bc_model_fit, pars=c("mu_linf", "mu_k", "mu_t0", "beta_linf", "beta_k", "beta_t0", "linf_lineage", "k_lineage", "t0_lineage"), probs=c(.1,.5,.9)) # None of the betas are sig
traceplot(bc_model_fit, pars = c("mu_linf", "mu_k", "mu_t0", "sigma"), inc_warmup = FALSE, nrow = 2)
traceplot(bc_model_fit, pars = c("beta_linf", "beta_k", "beta_t0"), inc_warmup = FALSE, nrow = 2)
traceplot(bc_model_fit, pars = c("Lcorr", "sigma_group"), inc_warmup = FALSE, nrow = 2)
traceplot(bc_model_fit, pars = c("Lcorr", "sigma_ind"), inc_warmup = FALSE, nrow = 2)

pairs(bc_model_fit, pars = c("mu_linf", "mu_k", "mu_t0", "lp__"), las = 1)
pairs(bc_model_fit, pars = c("linf_lineage", "k_lineage", "t0_lineage", "lp__"), las = 1)
pairs(bc_model_fit, pars = c("beta_linf", "beta_k", "beta_t0", "lp__"), las = 1)
pairs(bc_model_fit, pars = c("sigma_group", "Lcorr", "lp__"), las = 1)
pairs(bc_model_fit, pars = c("sigma_ind", "Lcorr", "lp__"), las = 1)

# BFMI low
pairs_stan <- function(chain, stan_model, pars) {
energy <- as.matrix(sapply(get_sampler_params(stan_model, inc_warmup = F), 
                           function(x) x[,"energy__"]))
pars <- extract(stan_model, pars = pars, permuted = F)
df <- data.frame(energy[,chain], pars[,chain,])
names(df)[1] <- "energy"

GGally::ggpairs(df, title = paste0("Chain", chain), 
                lower = list(continuous = GGally::wrap("points", alpha = 0.2)))                    
 }
 
pairs_stan(1, bc_model_fit, pars = c("mu_linf", "mu_k", "mu_t0", "lp__"))
pairs_stan(1, bc_model_fit, pars = c("linf_lineage", "k_lineage", "t0_lineage", "lp__"))
pairs_stan(1, bc_model_fit, pars = c("beta_linf", "beta_k", "beta_t0", "lp__"))
pairs_stan(1, bc_model_fit, pars = c("Lcorr", "lp__"))

check_energy(bc_model_fit)

# Rhat for chain convergence
rhats <- rhat(bc_model_fit)
color_scheme_set("brightblue") # see help("color_scheme_set")
mcmc_rhat(rhats)
rhats <- data.frame(Parm = names(rhats), Rhat = rhats)
```

#### 3b: Summarize mean and 95 credible intervals for all model coefficients; run the Rmd chunk below:

##### Summarize ancestry-condition fit model output:
```{r}
# Load back-calculated model fit output
load('data/growth_model_data/bc_model_fit.Rda')

# Convert model fit data to dataframe
bc_model_fit <- as.data.frame(bc_model_fit)

# select columns with parameter estimates to summarize for analysis, and calculate posterior differences for ancestry-specific parameters
bc_fixed <- bc_model_fit %>%
  select(mu_linf, mu_k, mu_t0, "linf_lineage[1]", "linf_lineage[2]", "k_lineage[1]", "k_lineage[2]", "t0_lineage[1]", "t0_lineage[2]", "beta_linf[1]", "beta_linf[2]", "beta_k[1]", "beta_k[2]", "beta_t0[1]", "beta_t0[2]") %>%
  mutate(linf_diff = `linf_lineage[1]` - `linf_lineage[2]`,
         k_diff = `k_lineage[1]` - `k_lineage[2]`,
         t0_diff = `t0_lineage[1]` - `t0_lineage[2]`)

# Gather parameters and estimates at each iteration so that there are only two columns
bc_fixed <- bc_fixed %>%
  gather(mu_linf:t0_diff, key = "parameter", value = "estimate") %>%
  mutate(parameter = factor(parameter))

# Summarize parameter estimates and 95% credible intervals
bc_fixed %>%
  group_by(parameter) %>%
  summarize(mean = mean(estimate),
            sd = sd(estimate),
            q2.5 = quantile(estimate, probs = 0.025),
            q5 = quantile(estimate, probs = 0.05), 
            q50 = quantile(estimate, probs = 0.50),
            q95 = quantile(estimate, probs = 0.95),
            q97.5 = quantile(estimate, probs = 0.975))
```





```{r}









# * Plot fitted model ----
cols <- c("#86BBD8","#2F4858", "#F6AE2D", "#F26419") # Colors for the VBGF lines (Females/Males)
draws <- as.data.frame(fit_backcalculated)


png( file = "growth_analysis/Figure2_backcalculated.png" , width=5, height = 5, family = "serif", units = "in", res = 300)
par( mar=c(3, 3 , 0.5 , 3) , oma=c(0 , 0 , 0 , 0), tcl = -0.35, mgp = c(1.75, 0.5, 0))

plot(y = length , x = age, 
     ylab = "Back-calculated TL (mm)", xlab = "Annuli", 
     cex = 2, cex.lab = 1.25,
     col = cols[full_bc_data$sex*2-1 + full_bc_data$river_code-1], 
     pch = c(17, 19)[full_bc_data$sex], 
     xlim = c(0, dat$Nages), ylim = c(0, 500))

# - Plot median curve
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[1,",colnames(draws))], 2, median), col = 1, lty = 1, lwd = 4) # Global

# - SMB Females
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[2,",colnames(draws))], 2, median), col = cols[1], lty = 1, lwd = 2) # Females river 1
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[4,",colnames(draws))], 2, median), col = cols[31], lty = 2, lwd = 2) # Females river 2

# - Neosho Females
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[6,",colnames(draws))], 2, median), col = cols[2], lty = 1, lwd = 2) # Females river 1
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[8,",colnames(draws))], 2, median), col = cols[2], lty = 2, lwd = 2) # Females river 2

# - SMB males
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[3,",colnames(draws))], 2, median), col = cols[3], lty = 1, lwd = 2) # Males river 1
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[5,",colnames(draws))], 2, median), col = cols[3], lty = 2, lwd = 2) # Males river 2

# - Neosho Males
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[7,",colnames(draws))], 2, median), col = cols[4], lty = 1, lwd = 2) # Males river 1
lines(1:dat$Nages, apply(draws[,grepl("pred_length\\[9,",colnames(draws))], 2, median), col = cols[4], lty = 2, lwd = 2) # Males river 2

legend("bottomright", c("SMB Females", "Neosho Females","SMB Males", "Neosho Males", "Big Sugar Creek", "Elk River"), col = c(cols,1,1), lty = c(1,1,1,1,1,2), bty = "n", lwd = 2)
dev.off()


# * Test Lineage Parameters #####
# - No sig difference
par(mfrow = c(2,3))
# - Posterior
hist(draws$`linf_lineage[1]`-draws$`linf_lineage[2]`, xlab = "Linf diff", main = NA)
hist(draws$`k_lineage[1]`-draws$`k_lineage[2]`, xlab = "K diff", main = "Back-calculated TL")
hist(draws$`t0_lineage[1]`-draws$`t0_lineage[2]`, xlab = "t0 diff", main = NA)

# - Prior
hist(draws$`prior_linf_lineage[1]`-draws$`prior_linf_lineage[2]`, xlab = "Linf diff", main = NA)
hist(draws$`prior_k_lineage[1]`-draws$`prior_k_lineage[2]`, xlab = "K diff")
hist(draws$`prior_t0_lineage[1]`-draws$`prior_t0_lineage[2]`, xlab = "t0 diff", main = NA)
par(mfrow = c(1,1))


# * Posterior prior plots ----
source("growth_analysis/Plot densities.R")
plot_density(fit_backcalculated, file_name = "growth_analysis/Figure3_backcalculated", probs = c(0.01, 0.99))


# * Summary ----
summ <- summary(fit_backcalculated, probs=c(.1,.5,.9))$summary
summ <- summary(fit_backcalculated, pars=c("mu_linf", "mu_k", "mu_t0", "linf_lineage", "post_linf_diff", "k_lineage", "post_k_diff", "t0_lineage", "post_t0_diff", "beta_linf", "beta_k", "beta_t0"), probs=c(.025, 0.05, .5, 0.95, .975))$summary # None of the betas are sig
summ <- as.data.frame(summ)
summ$parameter <- rownames(summ)
summ <- summ %>%
  select(parameter, mean, `2.5%`, `5%`, `50%`, `95%`, `97.5%`)
write.csv(summ, file = "growth_analysis/Table2_backcalculated_param.csv")


# - SMB - NB Females River 1 difference
length_at_age_diff <- draws[,grepl("pred_length\\[2,",colnames(draws))] - draws[,grepl("pred_length\\[6,",colnames(draws))]
colnames(length_at_age_diff) <- paste0("Difference at ", 1:8, " annuli")
length_at_age_diff <- rbind(
  apply(length_at_age_diff, 2, mean), 
  apply(length_at_age_diff, 2, function(x) quantile(x, probs=c(.025, 0.05, .5, 0.95, .975)))
)
length_at_age_diff <- t(length_at_age_diff)

# - Predicted length-at-age
length_at_age_smb <- draws[,grepl("pred_length\\[2,",colnames(draws))]
colnames(length_at_age_smb) <- paste0("SMB ",  1:8, " annuli")
length_at_age_smb <- rbind(
  apply(length_at_age_smb, 2, mean), 
  apply(length_at_age_smb, 2, function(x) quantile(x, probs=c(.025, 0.05, .5, 0.95, .975)))
)
length_at_age_smb <- t(length_at_age_smb)

length_at_age_n <- draws[,grepl("pred_length\\[6,",colnames(draws))]
colnames(length_at_age_n) <- paste0("NB ", 1:8, " annuli")
length_at_age_n <- rbind(
  apply(length_at_age_n, 2, mean), 
  apply(length_at_age_n, 2, function(x) quantile(x, probs=c(.025, 0.05, .5, 0.95, .975)))
)
length_at_age_n <- t(length_at_age_n)

write.csv(rbind(length_at_age_smb, length_at_age_n, length_at_age_diff), file = "growth_analysis/Table3_backcalculated_length_at_age.csv")


```

### ----------------------- END OF PHASE 2: VON BERTALANFFY GROWTH ANALYSIS ----------------------- ###

### ----------------------- END OF ANALYSIS 4: VON BERTALANFFY GROWTH ANALYSIS ----------------------- ###


