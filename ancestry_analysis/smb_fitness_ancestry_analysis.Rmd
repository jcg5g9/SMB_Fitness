---
title: "Analysis 3: Ancestry Inference and Genetic Group Assignment Analysis"
author: "Joe Gunn"
date: "2022-07-28"
output: html_document
---

# Project: Effects of admixture on fitness in Neosho Bass populations 
<font size="+1">We assessed the effect of admixture on fitness in two stream populations within the native range of the Neosho Bass (<i>M. velox</i>) which are known to have extensively hybridized with Smallmouth Bass (<i>Micropterus dolomieu</i>). Specifically, we used 14 microsatellite loci in a Bayesian analysis of population structure to estimate proportions of interspecific ancestry in individuals collected from Big Sugar Creek and the Elk River in southwestern Missouri (Central Interior Highlands ecoregion (CIH), North America). We used ancestry inference to identify fish as "Pure Neosho Bass", "Pure Smallmouth Bass", or "Admixed". For each group, we measured age and total length and projected individual growth using the standard parameterization of the von Berlanffy growth model, comparing average theoretical maximum length among groups. Finally, we used calculated a body condition as a proxy of fitness and generated heterozygosity-fitness correlations of body condition across the global dataset, within stream populations, and within ancestry groups. We ultimately sought to understand the short-term genetic consequences of admixture for Neosho Bass populations in order to better inform management and long-term viability of distinct, economically and ecologically important sportfish species in the CIH.</font>

## Specific Aim: Inference of individual ancestry proportions derived from Smallmouth Bass (SMB) and Neosho Bass (NB)
For this aim, we use Bayesian clustering analysis in the program STRUCTURE (see citation below in under "Programs Needed") to assess individual proportions of ancestry derived from Smallmouth Bass and Neosho Bass. We analyze all sample fish (obtained from Big Sugar Creek and Elk River within the Neosho Bass native range) together with reference fish (obtained from Crooked Creek, White River, and Tablerock Lake within the Smallmouth Bass native range). Knowing that Smallmouth Bass in the White River drainage are of pure genomic origin (Gunn et al. 2022), we use the minimum ancestry proportion of Smallmouth Bass derived from our microsatellites as a lower bound to identify "pure" vs. "admixed" fish in the Neosho Bass range. Ancestry groups are then used to assess growth and body condition in subsequent analyses.

## Phases of Analysis
### Phase 1: Ancestry inference
### Phase 2: Ancestry group assignment and visualization

Programs needed:

STRUCTURE v.2.3.4. (Pritchard et al. 2000)

Citation:

Pritchard JK, Stephens M, Donnelly P. 2000. Inference of population structure using multilocus genotype data. Genetics 155: 945-959.

CLUMPP v.1.1.2 (Jakobsson and Rosenberg 2007)

Citation:

Jakobsson M, Rosenberg NA. 2007. CLUMPP: A cluster matching and permutation program for dealing with label switching and multimodality in analysis of population structure. Bioinformatics 23: 1801-1806.

### Libraries needed for analysis
```{r}
library(tidyverse)
library(cowplot)
library(readxl)
library(writexl)
library(genepopedit)
library(adegenet)
```

## PHASE 1: ANCESTRY INFERENCE
In this phase of analysis, we use the software program STRUCTURE (see line 20 above) to assess individual ancestry in a Bayesian framework based on genotypes at 14 microsatellite loci. 

### STEP 1: Data preparation
In this step, we prepare all genotype data from the full, filtered dataset (`../filtering_analysis/data/processed_raw_data/full_data.rda`) for ancestry inference in the program STRUCTURE. We subset out all metadata and phenotype data and retain only unique sample IDs and microsatellite genotype information.

#### 1a: Load the fully filtered dataset ('full_data'); run the Rmd chunk below.

##### Load in full, filtered data:
```{r}
load("../filtering_analysis/data/processed_raw_data/full_data.rda")
```

#### 1b: Subset the data to include only individuals for the full population structure analysis and SPB diagnostic SNPs
In this step, we are sub-setting the fully filtered data to only include samples of interest for analysis of population structure and hybridization between SPB and the SMBC. For this analysis, these data will include all individuals across the whole dataset, but only SNP loci that were found to be diagnostic between SPB and the SMBC (SPB-SNPS)

##### 1b.1. Subset full data to include only sample_id and microsatellite genotypes; run the Rmd chunk below:

##### Filter out genotype data:
```{r}
## retain only sample_id column and genotype data
full_data <- full_data[,c(2,10:37)]
```

<b>Details on full genotype dataset:</b> <br>
<i>N</i><sub>samples</sub> = 136 <br>
<i>N</i><sub>loci</sub> = 14 <br>

#### 1c: Generate a new excel file with filtered genotype data; run the Rmd chunk below.

##### Generate Excel file, which is stored in the directory `data/processed_genotype_data/`
```{r}
## Generate excel files to hold genotype data
write_xlsx(full_data, path = "data/processed_genotype_data/genotype_data.xlsx")
```

#### 1d: Generate genepop file format in Excel.
In this step, we manually converted the Excel file generated in step 1c to basic Genepop format, carefully following the steps below:

##### 1d.1. Remove any metadata columns from the dataset except for "sample_id"

##### 1d.2. Add a "," after each sample_id

##### 1d.3. Designate different "populations" and insert a row above each new population in the data. Insert "pop" in the sample_id column in each empty row (the word "pop" should appear for each population)

For this analysis, we opted to treat each "stream" or "lake" as a distinct population, both in the sample data (Big Sugar Creek and Elk River) and reference data (Crooked Creek, White River, and Tablerock Lake). We therefore chose to split the data by the "FBS" ID series (Big Sugar Creek) and "FER" series (Elk River), and by samples in the reference populations known to occur within each river.

##### 1d.4. Concatenate alleles at each locus. The genotype file contains two columns for each microsatellite locus (28 columns total), one column with one allele, and the other column with a second allele, each a three-digit number. In Excel, concatenate each pair of alleles at each locus to generate only 14 total locus columns

##### 1d.4. Copy the header row of SNP IDs and transpose paste in a new Excel sheet; each SNP ID should be listed in its own row (number of rows should be equal to numbrer of SNP IDs)

##### 1d.6. Copy all genotype and label data from the original Excel file and paste in the row immediately below the list of SNP IDs

##### 1d.7. Save the Excel sheet as a .xlsx and a .txt file in a new folder: `data/genepop_data/genepop.txt`, etc.

#### 1e: Generate STRUCTURE formatted files from genepop format
In this step, we are reading in, checking, and converting genepop files for combined datasets into STRUCTURE formatted files. There are quite a few tricky and very specific formatting needs for the genepop text file before genepopedit will convert successfully. Follow Step 3a below closely to create a proper genepop format. Use Ctrl + F to find and replace the necessary things.

##### IMPORTANT NOTE ON DATA: genepopedit requires that sample names be in the format 'popname_01', 'popname_02', etc. where the 'popname' is a signifier for the population of origin for the sample, and the '01', etc. are numerical designations for the samples. 'popname' and the numeric must be separated by a "_", and there can only be a single "_". Otherwise, genepopedit will not read the sample's population of origin correctly. In the case of this project, we are clumping samples together by stream for STRUCTURE analysis to see if there is substructure mapped by stream. Many sample IDs in our dataset were not in the required format, so we added an additional column to the metadata set and associated processed data sets called "structure_ID", which gives a unique sample name in the proper structure format. This column can then be linked back to any other column in downstream analyses if we need to know which specific samples the results came from. 

##### 1e.1 Follow the guide for the library 'genepopedit' (Stanley et al. [year]) to prepare genepop files before converting them to STRUCTURE format.

##### 1e.1.1 All "," in the genepop text files were manually replaced with " ,  "

##### 1e.1.2 All "tabs" between six-digit alleles were were manually replaced with " " (a single space).

##### 1e.1.3 The top row designating the dataset combination was manually omitted.

##### 1e.1.4. Any additional space at the bottom of the text file was removed.

##### 1e.2. Generate STRUCTURE files from genepop files
Here it is important to not include the 'popgroup = ' unless it is explicitly necessary. Assigning popgroup to a dataframe with population names will cause the STRUCTURE input file to have word strings in the popdata column. Our experience is that STRUCTURE expects an integer here rather than a string, so best not to use popgroup.

##### Convert genepop to STRUCTURE format: files are stored in the directory `data/structure_data/`
```{r}
# Generate structure file
genepop_structure("data/genepop_data/genepop.txt", 
                  locusnames = TRUE, 
                  path = "data/structure_data/input_data/structure.txt")
```

#### 1f: Generate batch list files and shell scripts for running STRUCTURE in parallel
In this step, we are creating batch lists of command line code to run STRUCTURE analyses in parallel. Each batch list will contain a separate line of code to run a single replicate at an a priori determined number of populations (<i>K</i>, listed in the chunk above for each dataset). We will run each analysis in 10 replicates at each <i>K</i>, e.g.:

structure -K 1 -m mainparams -e extraparams -i structure_input.txt -o structure_output_1_1
structure -K 1 -m mainparams -e extraparams -i structure_input.txt -o structure_output_1_2
structure -K 1 -m mainparams -e extraparams -i structure_input.txt -o structure_output_1_3
structure -K 1 -m mainparams -e extraparams -i structure_input.txt -o structure_output_1_4
structure -K 1 -m mainparams -e extraparams -i structure_input.txt -o structure_output_1_5
.
.
.
structure -K 1 -m mainparams -e extraparams -i structure_input.txt -o structure_output_1_10
structure -K 2 -m mainparams -e extraparams -i structure_input.txt -o structure_output_2_1
structure -K 2 -m mainparams -e extraparams -i structure_input.txt -o structure_output_2_2
.
.
.
where, in the output name, the first number represents the <i>K</i> value for the run, and the last number represents the replicate.

##### 1f.1. Generate batch list file; run the Rmd chunk below.
The Rmd chunk below generates a list of commands that contains the full cluster paths (/home/data/...) for necessary files.

##### Generate batch commands: `code/batch_cmd_lists/structure_batch_cmd_list.txt`
```{r}
# Define number of "potential" populations in dataset
nk <- data.frame(c(1:5)) # There are 5 "pops" in the Genepop file

# Define the number of reps to run per K value
nreps <- data.frame(c(1:10))

cat("", file="code/batch_cmd_lists/structure_batch_cmd_list.txt")

# Run loop to create file for storing commands
for(ii in 1:nrow(nk)) {

  for(aa in 1:nrow(nreps)) {
    
    structure_call <- paste("structure ")

    param_files <- paste(" -m /home/jcg5g9/data/SMB_Fitness/ancestry_analysis/data/structure_data/param_files/mainparams -e /home/jcg5g9/data/SMB_Fitness/ancestry_analysis/data/structure_data/param_files/extraparams")

    input <- paste(" -i /home/jcg5g9/data/SMB_Fitness/ancestry_analysis/data/structure_data/input_data/structure.txt")
  
    output <- paste(" -o /home/jcg5g9/data/SMB_Fitness/ancestry_analysis/data/structure_data/output_data/structure_output/")
  
    cat(paste(structure_call, "-K ", nk[ii,], param_files, input, output, "structure_", nk[ii,], "_", nreps[aa,], sep=""),
      "\n", 
      file=paste("code/batch_cmd_lists/structure_batch_cmd_list.txt"),
      append=TRUE)
  }
}
```

##### 1f.2. Generate shell script; run the Rmd chunk below.

##### IMPORTANT NOTE: This shell script was generated on the server directly, and thus the Rmd chunk below does not need to be run to generate the file. This is purely to keep a record of each script file. 

##### Generate batch commands: `shell_scripts/structure.sh`
```{r}
#!/bin/bash
#-------------------------------------------------------------------------------
#  SBATCH CONFIG
#-------------------------------------------------------------------------------
## resources
#SBATCH --partition Lewis
#SBATCH --nodes=1
#SBATCH --ntasks=1  # used for MP#SBATCH -e error_%A_%a.err # Standard errorI codes, otherwise leav$
##SBATCH --cpus-per-task=12  # cores per task
#SBATCH --mem-per-cpu=16G  # memory per core (default is 1GB/core)
#SBATCH --time 2-00:00  # days-hours:minutes
#SBATCH --qos=normal
#SBATCH --array=23-220

## labels and outputs
#SBATCH --job-name=snolh_structure_jgunn
#
#SBATCH -o test_%A_%a.out # Standard output
#SBATCH -e error_%A_%a.err # Standard error

## notifications
#SBATCH --mail-user=jcg5g9@mail.missouri.edu  # email address for notifications
#SBATCH --mail-type=END,FAIL  # which type of notifications to send
#-------------------------------------------------------------------------------

#echo "### Starting at: $(date) ###"


# load packages
#module load rss/rss-2020
#module load structure/structure-2.3.4

#COMMANDA=`head -n ${SLURM_ARRAY_TASK_ID} ../batch_cmd_lists/spb_smbc_structure_batch_cmd_list.txt | tail -n 1`
#eval $COMMANDA


#echo "### Ending at: $(date) ###"
```

#### 1g: Prepare mainparam and extraparam files for STRUCTURE.
In this step, we are preparing the mainparam and extraparam input files for STRUCTURE so that they are unique to each analysis we are running.

##### 1g.1. Edit base mainparam STRUCTURE file (downloaded with STRUCTURE program) and generate a separate, unique mainparam file.

<b>mainparam file:</b> <br>
`../data/structure_data/param_files/mainparams` <br> 

###### 1g.1.1. Edit "maxpops" value to reflect the number of populations designated in Step 3b above. These values should be the following:

<b>Sample and reference populations:</b> 5 <br>

###### 1g.1.2. Set the number of burn-in and MCMC iterations to run; these are the same for each analysis:

<b>Burn-in runs:</b> 500,000 <br>
<b>MCMC runs:</b> 1,000,000 <br>

###### 1g.1.3. Set the number of individuals. These values should be the following:

<b>Individuals:</b> 136 <br>

###### 1g.1.4 Set the number of loci. These values should be the following:

<b>Loci:</b> 14 <br>

###### 1g.1.5. Set 'ONEROWPERIND' to '0'.

###### 1g.1.6. Set 'LABEL' to '1'.

###### 1g.1.7. Set 'POPDATA' to '1'.

###### 1g.1.8. Set 'POPFLAG' to '0'.

##### 1g.2. Edit base extraparam STRUCTURE file (downloaded with STRUCTURE program).
We did not change any of the default settings in the extraparams file (most importantly, we used the default admixture model), so we only used a single extraparams file.

<b>extraparam file:</b> <br>
`data/structure_data/param_files/extraparams` <br> 

### STEP 2: Population Structure Analysis with STRUCTURE.
See Line 22 for programs needed for analysis.

#### 2a: Run STRUCTURE analysis using the input data generated in STEP 1. Navigate to `shell_scripts/` Be sure that all relative and full paths to all input files and output destination directories are set up properly (ideally, this is already done within this GitHub repo). This command line code assumes capability to run the code using SLURM or a SLURM-like cluster scheduling software.

Run `sbatch structure.sh`

#### 2b: Structure output files are generated and stored here: `data/structure_data/output_data/structure_output/`.

#### 2c: Compress output directory into a zip file compatible with Structure Selector (Li and Liu 2017) or Structure Harvester (Earl and vonHoldt 2011) online.

#### 2d: Submit zip directory to Structure Selector or Structure Harvester to extract summary results.

#### 2e: Copy detlaK table (Evanno et al. 2005) and save as excel file here: `data/structure_data/summary_data/`
We further analyzed deltak results graphically in Step 2g.4.

#### 2f: Copy puechmaille table (Puechmaille et al. 2016) and save as excel file here:
`data/structure_data/summary_data/`
We found that Puechmaille metrics universally supported K=2, and we graph the results for deltaK and Puechmaille in Step 2g.5.

#### 2g: Visualize STRUCTURE runs for all data combinations.

##### 2g.1. Convert STRUCTURE files into aligned Q files compatible with analysis in the program CLUMPP (Jakobbson and Rosenberg 2007); run the rmd chunk below.

##### Convert STRUCTURE files to aligned Q files for CLUMPP: 
```{r}
# Get a list of structure files for each data combination
s <- list.files("data/structure_data/output_data/structure_output/",
                full.names = T)

# Extract q value information (ancestry proportions) from each run for each K value for each individual from the STRUCTURE output files in the directories listed above
Q <- readQ(s)

# Tabulate information from the q lists
tab <- tabulateQ(Q)

# Summarize information from tabultions above
summary <- summariseQ(tab)

# Extract deltaK and associated summary information using Evanno method
evanno <- evannoMethodStructure(summary, returnplot = F)

# Set infinity and NA to zero arbitrarily
evanno$deltaK[evanno$deltaK == "Inf"] <- 0
evanno$deltaK[is.na(evanno$deltaK)] <- 0

# Write Evanno table to Excel table for manuscript preparation.

## Convert to data frame
evanno <- as.data.frame(evanno)

## Write Excel file
write_xlsx(evanno, "data/structure_data/summary_data/deltak.xlsx")

# Align replicate runs for each K to correct label switching
align <- alignK(Q)
```

##### 2g.2. Export CLUMPP compatible files for CLUMPP analysis; run the rmd chunk below to export CLUMPP associated files for each K.

<b>We used the following parameters: </b> <br>

Large-K-Greedy algorithm (paramrep = 3) <br>
10,000 replicates <br>

### IMPORTANT NOTE: This step only needs to be run ONCE to generate files for CLUMPP. Once you have run this chunk, move on to Step 2g.3. Uncomment each line to run this code.

##### Export CLUMPP files:
```{r}
clumppExport(align, 
            parammode = 3, 
            paramrep = 10000,
            exportpath = "data/structure_data/clumpp_data")
```

The code above generates a .txt file (pop_K#-combined.txt) with combined cluster ancestry proportions for each individual at each K and stores it in a separate directory labeled "pop_K#", where '#' is the corresponding K value. Additionally, the code generates an accompanying "paramfile" that is input for CLUMPP, which contains information on parameters to generate merged datasets across our 10 replicates at each K.

##### 2g.3. Generate merged Q files using CLUMPP.
In this step, we used the software program CLUMPP (Jakobbson and Rosenberg 2007) to merge cluster ancestry proportion inferences for all replicate runs of each K across individuals. We downloaded the Linux 64-bit build of CLUMPP v.1.1.2 and installed it on our computing cluster (University of Missouri). We then executed the program for each analysis by sequentially copying the associated paramfiles generated in the clumppExport function (see section above) to the CLUMPP home directory. All output files were then moved to the corresponding structure directory.

###### 2g.3.1. Downolad CLUMPP and install on cluster; navigate to the desired directory: `code`. Run the command: `wget https://rosenberglab.stanford.edu/software/CLUMPP_Linux64.1.1.2.tar.gz`

###### 2g.3.2. Run the command: `gunzip CLUMPP_Linux64.1.1.2.tar.gz`

###### 2g.3.3. Run the command: `tar -xvf CLUMPP_Linux64.1.1.2.tar`

###### 2g.3.4. For each analysis separately, copy the 'pop_K#-combined.txt' file and 'paramfile' over to the CLUMPP directory (`CLUMPP_Linux64.1.1.2`)

###### 2g.3.5. For each analysis separately, execute CLUMPP on the paramfile; run the code `./CLUMPP paramfile`
We are running this code with GREEDY OPTION 2, which uses random sampling of replicate runs. 

This code generates three additional files, which we moved back to the corresponding pop_K folder in the clumpp_data directory:

<b>Files generated: </b> <br>

pop_K#-combined-miscfile.txt <br>
pop_K#-combined-merged.txt <br>
pop_K#-combined-aligned.txt <br>

##### 2g.4. Plot deltaK results; run the rmd chunk below.

##### Plot deltak results: `deltaK.pdf` 
```{r}
# DeltaK results
pdf("figures/deltaK.pdf", width=8, height=5)

ggplot(evanno, aes(x=k, y=deltaK)) +
  geom_point() + 
  geom_line() +
  theme_cowplot(theme_set(12)) +
  geom_vline(xintercept = 2, color = "blue") +
  theme(axis.text = element_text(size = 15)) +
  theme(axis.title = element_text(size = 15)) +
  theme(axis.title.x = element_text(face = "italic")) +
  labs(x = "K", y = "deltaK") + 
  scale_x_continuous("K", labels = as.character(evanno$k), breaks = evanno$k)

dev.off()
```

We found strongest support for K=2 using the deltaK metric. We also found strongest support for K=2 using the Puechmaille metric. We therefore present result for K=2 in Phase 2 below. Results for deltaK and Puechmaille metrics are given in Tables S2 and S3, respectively.

#### 2e. Gather ancestry coefficient data from STRUCTURE results and merge with metadata for downstream analyses.
In this step, we collect Smallmouth Bass and Neosho Bass ancestry coefficients for each individual sample, and we combine with metadata for future analyses.

##### 2e.1. Isolate ancestry coefficients from STRUCTURE data; run the Rmd chunk below.

##### Isolate ancestry coefficient data:
```{r}
# Read in merged runs from STRUCTURE (file generated in CLUMPP)
ancestry <- read.table("data/structure_data/clumpp_data/pop_K2/pop_K2-combined-merged.txt", header = F)

# Remove first and last columns (they are not needed)
ancestry <- ancestry[,-c(1,4)]

## Bind Structure Number metadata to ancestry data
# Load in saved metadata
load("../filtering_analysis/data/processed_raw_data/metadata.Rda")

# Bind metadata to ancestry data
ancestry <- cbind(metadata$structure_number, 
                  ancestry)

# Name columns based on majority ancestry
colnames(ancestry) <- c("structure_number", "nb", "smb")

# Save processed ancestry data
save(ancestry, file = "data/structure_data/ancestry_data/ancestry.Rda")
```

### ----------------------- END OF PHASE 1: ANCESTRY INFERENCE ----------------------- ###

## PHASE 2: ANCESTRY GROUP ASSIGNMENT AND VISUALIZATION
In this phase of analysis, we use the individual ancestry coefficients for Smallmouth Bass and Neosho Bass calculated in STRUCTURE (see Phase 1 above) to assign each fish in the Neosho range (Big Sugar Creek and Elk River) to one of three ancestry groups: 1) pure Smallmouth Bass, 2) pure Neosho Bass, or 3) Admixed. We visualize ancestry coefficients and ancestry group assignment proportions in each river, and we use group assignments for growth and heterozygosity-fitness correlation analysis in Analyses 4 and 5, respectively.

### STEP 1: Order ancestry data for effective plotting; run the Rmd chunk below.
In this step, we arrange samples with the Smallmouth Bass range (reference samples) and samples within the Neosho Bass range (Big Sugar Creek and Elk River samples) in order of Smallmouth Bass ancestry coefficient for effective plotting.

##### Arrange Smallmouth Bass reference samples and Neosho Bass samples separately in order of Smallmouth Bass ancestry coefficient:
```{r}
# Load in processed ancestry data
load("data/structure_data/ancestry_data/ancestry.Rda")

# Separate out and create data frame for Neosho Bass samples only
ancestry_nb <- ancestry[c(1:116),]

# Name columns based on majority ancestry
colnames(ancestry_nb) <- c("structure_number", "nb", "smb")

# Arrange samples in order of Smallmouth Bass ancestry
ancestry_nb <- ancestry_nb %>%
  arrange(desc(smb))

# Separate out and create data frame for Smallmouth Bass samples only
ancestry_smb <- ancestry[c(117:136),]

# Name columns based on majority ancestry
colnames(ancestry_smb) <- c("structure_number", "nb", "smb")

# Arrange samples in order of Smallmouth Bass ancestry
ancestry_smb <- ancestry_smb %>%
  arrange(desc(smb))

# Combine (rbind) nb and smb samples in their arranged order
ancestry_ordered <- rbind(ancestry_smb, 
                  ancestry_nb)

# Create rank order column for later plotting (column to give the order of plotting, from 1 to 136)
ancestry_ordered$rank_order <- as.numeric(c(1:136))

# Load in processed metadata from filtering analysis
load("../filtering_analysis/data/processed_raw_data/metadata.Rda")

# Merge rank-ordered data with metadata so that the rank_order column appears in metadata
ancestry_metadata <- merge(metadata, 
                           ancestry_ordered,
                           by = "structure_number")

# Save ancestry metadata for downstream analyses
save(ancestry_metadata, file = "data/structure_data/ancestry_data/ancestry_metadata.Rda")
```

### STEP 2: Plot ancestry coefficients for all individuals. 

#### 2a. Plot ancestry coefficients for all individuals at best K value (K=2) inferred in STRUCTURE; run the rmd chunk below.

##### STRUCTURE results: `k2.pdf`
```{r}
# Load in ancestry metadata for proper sample ordering in STRUCTURE plot
load("data/structure_data/ancestry_data/ancestry_metadata.Rda")

# Omit unneeded columns
ancestry_metadata <- ancestry_metadata[,c(1,10:12)]

# Gather dataset for plotting and convert variables to factors/characters. 'structure_number' was converted to a character in this step, and then factor levels were set in the next step to ensure that individuals were graphed in the same order as in STRUCTURE (for later direct one-to-one comparison)
ancestry_metadata <- ancestry_metadata %>%
  gather(c(2:3), key = "ancestry", value = "q") %>%
  mutate(ancestry = factor(ancestry)) %>%
  mutate(rank_order = as.character(rank_order))

# Change levels to get the correct order (see explanation for step above)
ancestry_metadata$rank_order <- factor(ancestry_metadata$rank_order, levels = c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136"))

# Convert analysis order column to factor
ancestry_metadata <- ancestry_metadata %>%
  mutate(structure_number = factor(structure_number))

# Reorder analysis order to be in the same order as Rivers (same as presented in Structure q plot)
ancestry_metadata <- ancestry_metadata %>% 
  mutate(structure_number = fct_reorder(structure_number, as.integer(rank_order)))

# Plot posterior probability of assignment to each hybrid category as a cumulative bar plot for each individual (colors represent assignment probabilities for each of six possible hybrid categories)

## we include black dashed lines at q=0.25 and q=0.75 to show thresholds for group membership.
pdf("figures/k2.pdf", width=140, height=12)

ggplot(ancestry_metadata, aes(x = structure_number, y = q, fill = ancestry)) + 
  geom_bar(stat = "identity", show.legend = T, color = "black") +
  geom_hline(yintercept = 0.25, color = "black", linetype = "longdash", size = 4) +
  geom_hline(yintercept = 0.75, color = "black", linetype = "longdash", size = 4) +
  theme_set(theme_cowplot(12)) +
  labs(x = "Individual", y = "q") +
  scale_fill_manual(values = c("deepskyblue", "deeppink2")) +
  theme(axis.title.x = element_blank())  +
  theme(panel.border = element_rect(colour = "black", fill=NA, size=1)) +
  theme(axis.text = element_text(angle = 90)) +
  scale_y_continuous(expand=c(0,0))

dev.off()
```

This plot is the basis for Figure S2a in the final manuscript.

### STEP 3: Assign individuals to genetic groups based on ancestry proportions and summarize sample sizes for each group.
In this step, we assign individual fish to one of three "ancestry" groups based on the membership coefficients calculated in STRUCTURE: 1) Pure Smallmouth Bass (<i>q</i><sub>SMB</sub> = 0.75 or greater), 2) Pure Neosho Bass (<i>q</i><sub>NB</sub> = 0.75 or greater), or 3) Admixed (<i>q</i><sub>SMB</sub> = 0.25-0.75). 

We chose to set 0.75 as a threshold for ancestry group, given that first generation back-crossed individuals would be expected to possess 75% of their ancestry from one species and 25% to the other. Greater than 75% ancestry could therefore indicate there's a greater probability of pure ancestry in that individual.

#### 3a. Generate a new column in the ancestry metadata set assigning individuals to their respective ancestry group based on membership coefficient; run the Rmd chunk below.

##### Assign individuals to ancestry group and add data to metadata file.
```{r}
## Load in ancestry metadata (un-manipulated as in Step 2 above)
load("data/structure_data/ancestry_data/ancestry_metadata.Rda")

# Categorize sample ids into ancestry groups based on membership coefficient
ancestry_metadata$ancestry_group <- with(ancestry_metadata, ifelse(smb > 0.75, "Smallmouth_Bass",
                                             ifelse(nb > 0.75, "Neosho_Bass",
                                                    ifelse(smb <= 0.75 & nb <= 275, "Admixed", "done"))))
```

#### 3b. Merge ancestry metadata with full dataset (see Analysis 2) and save for downstream analyses; run the Rmd chunk below.

##### Summarize ancestry group data:
```{r}
# Load in full dataset
load("../filtering_analysis/data/processed_raw_data/full_data.Rda")

# Merge full data with ancestry metadata
full_ancestry_data <- merge(full_data, 
                   ancestry_metadata, 
                   by = c("sample_id", 
                          "structure_number", 
                          "river_code",
                          "sample_number", 
                          "easting", 
                          "northing", 
                          "range_id", 
                          "river",
                          "population"))

# Save full ancestry data for all downstream analyses
save(full_ancestry_data, file = "data/processed_ancestry_data/full_ancestry_data.Rda")
```

#### 3c. Summarize ancestry data by quantifying sample sizes per group; run the Rmd chunk below.
In this step, we count sample sizes for each ancestry group and present them in Table 1, along with sample sizes per length category and stream (Big Sugar Creek or Elk River).

##### Summarize ancestry group data:
```{r}
# Convert ancestry group variable to factor
full_data <- full_data %>%
  mutate(ancestry_group = factor(ancestry_group))

# Count total number of fish per ancestry group
full_data %>% 
  group_by(ancestry_group) %>%
  count()

# Summarize full data by river within ancestry group
full_data %>% 
  group_by(ancestry_group, river) %>%
  count()

# Summarize full data by size bin for sex within river
full_data %>% 
  group_by(size_bin, river, ancestry_group) %>%
  count()
```

### Data summary:

## Totals by ancestry group 
<b><i>N</i><sub>Neosho_Bass</sub> = 46 </b><br>
<i>N</i><sub>Smallmouth Bass</sub> = 34 <br>
<i>N</i><sub>Admixed</sub> = 56 <br>

## By river (wihtin ancestry group)
<i>N</i><sub>adm_elk</sub> = 34 <br>
<i>N</i><sub>adm_bigsugar</sub> = 20 <br>

<i>N</i><sub>nb_elk</sub> = 26 <br>
<i>N</i><sub>nb_bigsugar</sub> = 20 <br>

<i>N</i><sub>smb_elk</sub> = 10 <br>
<i>N</i><sub>smb_bigsugar</sub> = 6 <br>

## By size bin within sex within river
<i>N</i><sub>bigsugar_200-225_nb</sub> = 4 <br>
<i>N</i><sub>bigsugar_200-225_smb</sub> = 2 <br>
<i>N</i><sub>bigsugar_200-225_adm</sub> = 4 <br>

<i>N</i><sub>bigsugar_225-250_nb</sub> = 3 <br>
<i>N</i><sub>bigsugar_225-250_smb</sub> = 0 <br>
<i>N</i><sub>bigsugar_225-250_adm</sub> = 2 <br>

<i>N</i><sub>bigsugar_250-275_nb</sub> = 1 <br>
<i>N</i><sub>bigsugar_250-275_smb</sub> = 1 <br>
<i>N</i><sub>bigsugar_250-275_adm</sub> = 1 <br>

<i>N</i><sub>bigsugar_275-300_nb</sub> = 3 <br>
<i>N</i><sub>bigsugar_275-300_smb</sub> = 2 <br>
<i>N</i><sub>bigsugar_275-300_adm</sub> = 3 <br>

<i>N</i><sub>bigsugar_300-325_nb</sub> = 5 <br>
<i>N</i><sub>bigsugar_300-325_smb</sub> = 1 <br>
<i>N</i><sub>bigsugar_300-325_adm</sub> = 5 <br>

<i>N</i><sub>bigsugar_325-350_nb</sub> = 1 <br>
<i>N</i><sub>bigsugar_325-350_smb</sub> = 0 <br>
<i>N</i><sub>bigsugar_325-350_adm</sub> = 3 <br>

<i>N</i><sub>bigsugar_350-375_nb</sub> =  1c<br>
<i>N</i><sub>bigsugar_350-375_smb</sub> = 0 <br>
<i>N</i><sub>bigsugar_350-375_adm</sub> = 0 <br>

<i>N</i><sub>bigsugar_375-400+_nb</sub> = 2 <br>
<i>N</i><sub>bigsugar_375-400+_smb</sub> = 0 <br>
<i>N</i><sub>bigsugar_375-400+_adm</sub> = 2 <br>

<i>N</i><sub>elk_200-225_nb</sub> = 3 <br>
<i>N</i><sub>elk_200-225_smb</sub> = 2 <br>
<i>N</i><sub>elk_200-225_adm</sub> = 5 <br>

<i>N</i><sub>elk_225-250_nb</sub> = 6 <br>
<i>N</i><sub>elk_225-250_smb</sub> = 2 <br>
<i>N</i><sub>elk_225-250_adm</sub> = 6 <br>

<i>N</i><sub>elk_250-275_nb</sub> = 6 <br>
<i>N</i><sub>elk_250-275_smb</sub> = 3 <br>
<i>N</i><sub>elk_250-275_adm</sub> = 8 <br>

<i>N</i><sub>elk_275-300_nb</sub> = 6 <br>
<i>N</i><sub>elk_275-300_smb</sub> = 1 <br>
<i>N</i><sub>elk_275-300_adm</sub> = 5 <br>

<i>N</i><sub>elk_300-325_nb</sub> = 2 <br>
<i>N</i><sub>elk_300-325_smb</sub> = 0 <br>
<i>N</i><sub>elk_300-325_adm</sub> = 5 <br>

<i>N</i><sub>elk_325-350_nb</sub> = 0 <br>
<i>N</i><sub>elk_325-350_smb</sub> = 1 <br>
<i>N</i><sub>elk_325-350_adm</sub> = 2 <br>

<i>N</i><sub>elk_350-375_nb</sub> = 0 <br>
<i>N</i><sub>elk_350-375_smb</sub> = 0 <br>
<i>N</i><sub>elk_350-375_adm</sub> = 1 <br>

<i>N</i><sub>elk_375-400+_nb</sub> = 3 <br>
<i>N</i><sub>elk_375-400+_smb</sub> = 1 <br>
<i>N</i><sub>elk_375-400+_adm</sub> = 2 <br>

These sample sizes were used as the basis for Table 1b.

### STEP 4. Validate ancestry group assignment using Discriminant Analysis of Principal Components (DAPC).
In this step, we validate the group assignments used in STEP 3 above using DAPC.

#### 4a. Read in genotype data and format for DAPC (convert to genind object for DFA); run the Rmd chunk below.
In this step, we format the genotype data (two columns per locus, each column containing a three digit code for each allele at a locus) for analysis in DAPC. The dapc() function in the adegenet package requires genotypes to be in genind format, so we prepare the genotype file for conversion using the df2genind() function.

##### Convert genotypes to genind format:
```{r}
## Load in full ancestry data
load("data/processed_ancestry_data/full_ancestry_data.Rda")

# Read in the fully processed genotype data
genotypes <- read_excel("data/processed_genotype_data/genotype_data.xlsx")

# Concatenate alleles for each locus (each locus has 2 columns in the original dataset, each with one allele at that locus)
genotypes <- genotypes %>%
  unite("mdo9", c(2:3), sep = "") %>%
  unite("mdo5", c(3:4), sep = "") %>%
  unite("mdo7", c(4:5), sep = "") %>%
  unite("mdo10", c(5:6), sep = "") %>%
  unite("mdo6", c(6:7), sep = "") %>%
  unite("mdo3", c(7:8), sep = "") %>%
  unite("mdo2", c(8:9), sep = "") %>%
  unite("lma21", c(9:10), sep = "") %>%
  unite("msaf29", c(10:11), sep = "") %>%
  unite("msaf05", c(11:12), sep = "") %>%
  unite("msaf14", c(12:13), sep = "") %>%
  unite("msaf17", c(13:14), sep = "") %>%
  unite("msaf09", c(14:15), sep = "") %>%
  unite("msaf06", c(15:16), sep = "")

# Remove the first column
genotypes <- genotypes[,-c(1)]

# Create factor for ancestry group
ancestry_group <- factor(full_ancestry_data$ancestry_group)

# Convert genotypes to genind object
genind <- df2genind(genotypes,
                    ncode = 3, 
                    pop = ancestry_group, 
                    ploidy = 2)

# Scale genind object, filling NA's with imputed means
genind <- scaleGen(genind, 
                   center = TRUE, 
                   scale = TRUE,
                   NA.method = c("mean"), 
                   truenames = TRUE)

# Save genind for downstream processing and plotting
save(genind, file = "data/processed_ancestry_data/genind.Rda")
```

#### 4b. Run 10-fold cross-validation on genotype data; run the Rmd chunk below.
In this step, we run a cross-validation analysis, using 90% of samples in the data as a training set and 10% of data as a test set, maximum number of pcs at 100, and 30 replicates per PC value tested, to choose the number of PCs to use in each analysis. We choose the number of PCs that maximizes successful assignment to our <i>a priori</i> defined populations, which will then give us the optimal clustering patterns of the data.

##### Run 10-fold cross-validation and generate plot: `dapc_xval.pdf`
```{r}
# Load in genind object for analysis
load("data/processed_ancestry_data/genind.Rda")

# Run 10-fold cross-validation
xval <- xvalDapc(genind, 
                 ancestry_group, 
                 n.pca.max = 100, 
                 training.set = 0.9,
                 result = "groupMean", 
                 center = TRUE, 
                 scale = FALSE,
                 n.pca = NULL,
                 n.rep = 30, 
                 xval.plot = TRUE)

# Get xval data 
xval_data <- as.data.frame(xval$`Cross-Validation Results`) 

# Get mean of assignment success for plotting
xval_aves <- xval_data %>%
  group_by(n.pca) %>%
  summarize(mean_success = mean(success))

# Get standard deviation of assignment success for plotting
xval_sd <- xval_data %>%
  group_by(n.pca) %>%
  summarize(sd_success = sd(success))

# Merge averages and standard deviations for plotting
xval_data <- merge(xval_aves,
                   xval_sd, 
                   by = "n.pca")

# Save xval data for plotting
save(xval_data, file = "data/processed_ancestry_data/xval_data.Rda")

# Load in xval data for plotting
load("data/processed_ancestry_data/xval_data.Rda")

# Plot cross validation results for SPB vs. SMBC
pdf("figures/dapc_xval.pdf", width=8, height=6)

ggplot(xval_data, aes(x = n.pca, y = mean_success)) +
  geom_errorbar(aes(ymin = mean_success - sd_success, ymax = mean_success + sd_success), width=1) +
  geom_point(fill = "grey", color = "black", pch = 21, size = 5, show.legend = F) +
  theme_set(theme_cowplot(12)) +
  labs(x = "PCs retained", y = "Average CV success (%)") +
  theme(axis.text = element_text(size = 20)) +
  theme(axis.title = element_text(size = 20)) +
  scale_x_continuous("PCs retained", labels = as.character(xval_data$n.pca), breaks = xval_data$n.pca) +
  theme(panel.border = element_rect(colour = "black", fill=NA, size=1))

dev.off()
```

This plot is the basis for Figure S2b in the final manuscript.

We found that the optimal number of PCs for DAPC (based on 10-fold cross-validation analysis) is 30. We therefore input 30 as the number of PCs to retain for analysis.

#### 4c. Run DAPC analysis and plot results; run the Rmd chunk below.
In this step, we run DAPC analysis and plot using built-in functions in the R package "adegenet."

##### Run DAPC and plot: 1) `figures/dapc.pdf` and 2) `figures/dapc_legends.pdf`
```{r}
# Load in genind object for analysis
load("data/processed_ancestry_data/genind.Rda")

## Load in full ancestry data
load("data/processed_ancestry_data/full_ancestry_data.Rda")

# Create factor for ancestry group
ancestry_group <- factor(full_ancestry_data$ancestry_group)

# Run DAPC at inferred optimal number of PCs
dapc <- dapc(genind, ancestry_group)

# We selected 30 PCs and 2 Discriminant Functions to retain (see figure inset)

#Save dapc for future plotting
save(dapc, file = "data/processed_ancestry_data/dapc.Rda")

# Choose color palette for plotting.
color_palette <- c("mediumpurple","deepskyblue","deeppink2")

## Generate two plots: 1) genotype data in DAPC WITHOUT LEGENDS (We created a second figure with labels so they could be cut and pasted into the first figure, giving more control over placement and aesthetics); and 2) genotype data WITH LEGENDS

# Plot genetic data WITHOUT LEGENDS
pdf("figures/dapc.pdf", width = 5, height = 3)

scatter(dapc, 
        legend = F,
        col = color_palette, 
        clab = 0, 
        cstar = 1,
        cex = 2,
        scree.pca = F,
        posi.pca = "topleft",
        scree.da = F,
        posi.da = "topright",
        cell = 1)

dev.off()

# Plot genetic data WITH LEGENDS
pdf("figures/dapc_legends.pdf", width = 10, height = 10)

scatter(dapc, 
        legend = F,
        col = color_palette, 
        clab = 0, 
        cstar = 1,
        cex = 2,
        scree.pca = T,
        posi.pca = "topleft",
        scree.da = T,
        posi.da = "topright",
        cell = 1)

dev.off()
```

These plots are the basis for Figure S2c in the final manuscript.

### ----------------------- END OF PHASE 2: ANCESTRY GROUP ASSIGNMENT AND VISUALIZATION ----------------------- ###

### ----------------------- END OF ANALYSIS 2: ANCESTRY INFERENCE AND GENETIC GROUP ASSIGNMENT ANALYSIS ----------------------- ###


